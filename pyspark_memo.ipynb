{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Initializing Spark: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# create spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "# reduce logs in the shell mode\n",
    "#sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('project-capstone/Twitter_sentiment_analysis/clean_tweet.csv')\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Actions and transformations: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformations: create a new RDD from the existing one\n",
    "- Actions: return a value to the driver program after running a computation on the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> Transformations are __lazy__, they __do not compute their results RIGHT AWAY__, they are only computed when an __action__ is done. Hence, some errors can be done without notification while doing only transformations.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following color code is used to make differnce between <font color ='blue' size =4px> transformations </font> and <font color ='#E2AC00' size =4px> actions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'> Creating RDD from python collection: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"RDDs are the building blocks of Spark, it is the original API that Spark exposed and pretty much all the higher level APIs decompose to RDDs.\n",
    "From a developer perspective, an RDD is simply a set of Java or Scala objects representing data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(15151, 25463,35000, 45000), \n",
    "     (61612, 68645,61600, 55000), \n",
    "     (1615, 5463,15151, 25463), \n",
    "     (25000, 15000,10000, 5000), \n",
    "     (151, 263,342, 512)]\n",
    "rdd=sc.parallelize(l, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15151, 25463, 35000, 45000), (61612, 68645, 61600, 55000), (1615, 5463, 15151, 25463), (25000, 15000, 10000, 5000), (151, 263, 342, 512)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Applying Functions to each RDD element: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> map: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors, DenseVector\n",
    "\n",
    "# apply the transformation (the lambda function) to each item of the rdd\n",
    "elements = rdd.map(lambda row : Vectors.dense([row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15151, 25463, 35000, 45000),\n",
       " (61612, 68645, 61600, 55000),\n",
       " (1615, 5463, 15151, 25463),\n",
       " (25000, 15000, 10000, 5000),\n",
       " (151, 263, 342, 512)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([15151.0, 25463.0, 35000.0, 45000.0]),\n",
       " DenseVector([61612.0, 68645.0, 61600.0, 55000.0]),\n",
       " DenseVector([1615.0, 5463.0, 15151.0, 25463.0]),\n",
       " DenseVector([25000.0, 15000.0, 10000.0, 5000.0]),\n",
       " DenseVector([151.0, 263.0, 342.0, 512.0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> flatMap:</font>\n",
    "Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix= rdd.map(lambda row : [row[0],row[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15151, 25463], [61612, 68645], [1615, 5463], [25000, 15000], [151, 263]]\n"
     ]
    }
   ],
   "source": [
    "# the result is a list of lists\n",
    "print(matrix.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector= rdd.flatMap(lambda row : [row[0],row[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15151, 25463, 61612, 68645, 1615, 5463, 25000, 15000, 151, 263]\n"
     ]
    }
   ],
   "source": [
    "# the result is a list\n",
    "print(vector.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> map with a class function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(lambda s: self.field + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doStuff(self, rdd):\n",
    "    field = self.field\n",
    "    return rdd.map(lambda s: field + s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Filtering: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> filter:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids=sc.parallelize([(0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
    "                          (1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter the element that corresponds to the condition x[0]==1\n",
    "c1=centroids.filter(lambda x:x[0]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))]\n"
     ]
    }
   ],
   "source": [
    "print(c1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Getting: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> collect:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns all RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([15151.0, 25463.0, 35000.0, 45000.0]),\n",
       " DenseVector([61612.0, 68645.0, 61600.0, 55000.0]),\n",
       " DenseVector([1615.0, 5463.0, 15151.0, 25463.0]),\n",
       " DenseVector([25000.0, 15000.0, 10000.0, 5000.0]),\n",
       " DenseVector([151.0, 263.0, 342.0, 512.0])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> avoid collect for __large RDDs__, avoid __print( RDD.collect() )__ assign it to a variable or save it into a file instead !!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> if not the entire RDD is needed, use take,sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> take:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD.take(n) returns first n RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([15151.0, 25463.0, 35000.0, 45000.0]),\n",
       " DenseVector([61612.0, 68645.0, 61600.0, 55000.0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> sample:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD.sample(withReplacement, fraction, seed=None) Return a new RDD containing a statistical sample of the original RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([61612.0, 68645.0, 61600.0, 55000.0]),\n",
       " DenseVector([25000.0, 15000.0, 10000.0, 5000.0]),\n",
       " DenseVector([151.0, 263.0, 342.0, 512.0])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.sample(False,0.5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Reshaping data: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'>  groupBy:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path=sc.parallelize([(0,[(0, 1), (1, 1), (2, 2), (3, 3)]), \n",
    "                     (1, [(0, 2), (3, 1), (2, 2), (3, 3)]), \n",
    "                     (1, [(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)]), \n",
    "                     (0, [(1, 0), (0, 1), (2, 2), (3, 3)]), \n",
    "                     (0, [(0, 1), (1, 0), (2, 0), (3, 1), (3, 2), (3, 3)])],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0), <pyspark.resultiterable.ResultIterable at 0x9a4c400>),\n",
       " ((0, 2), <pyspark.resultiterable.ResultIterable at 0x9a4ceb8>),\n",
       " ((0, 1), <pyspark.resultiterable.ResultIterable at 0x9a4c710>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will group the elements which have the same tuple in the first index, the result is (common_key,ResultIterable)\n",
    "path.groupBy(lambda x:x[1][0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the ResultIterable, convert it into collection (list,tuple,dict...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0), [(0, [(1, 0), (0, 1), (2, 2), (3, 3)])]),\n",
       " ((0, 2),\n",
       "  [(1, [(0, 2), (3, 1), (2, 2), (3, 3)]),\n",
       "   (1, [(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)])]),\n",
       " ((0, 1),\n",
       "  [(0, [(0, 1), (1, 1), (2, 2), (3, 3)]),\n",
       "   (0, [(0, 1), (1, 0), (2, 0), (3, 1), (3, 2), (3, 3)])])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.groupBy(lambda x:x[1][0]).map(lambda x:(x[0],list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply functions directly to the iterable (len, max, min,...) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0), 1), ((0, 2), 2), ((0, 1), 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.groupBy(lambda x:x[1][0]).map(lambda x:(x[0],len(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> groupBy does not ALWAYS keep keys order and elements order for the same key !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attempt1=path.groupBy(lambda x:x[1][0]).map(lambda x:(x[0],list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attempt10=path.groupBy(lambda x:x[1][0]).map(lambda x:(x[0],list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0), [(0, [(1, 0), (0, 1), (2, 2), (3, 3)])]),\n",
       " ((0, 2),\n",
       "  [(1, [(0, 2), (3, 1), (2, 2), (3, 3)]),\n",
       "   (1, [(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)])]),\n",
       " ((0, 1),\n",
       "  [(0, [(0, 1), (1, 1), (2, 2), (3, 3)]),\n",
       "   (0, [(0, 1), (1, 0), (2, 0), (3, 1), (3, 2), (3, 3)])])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attempt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0), [(0, [(1, 0), (0, 1), (2, 2), (3, 3)])]),\n",
       " ((0, 2),\n",
       "  [(1, [(0, 2), (3, 1), (2, 2), (3, 3)]),\n",
       "   (1, [(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)])]),\n",
       " ((0, 1),\n",
       "  [(0, [(0, 1), (1, 1), (2, 2), (3, 3)]),\n",
       "   (0, [(0, 1), (1, 0), (2, 0), (3, 1), (3, 2), (3, 3)])])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attempt10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attempt1==attempt10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> groupByKey:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups the elements having the same key together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ([(0, 1), (1, 1), (2, 2), (3, 3)],\n",
       "   [(1, 0), (0, 1), (2, 2), (3, 3)],\n",
       "   [(0, 1), (1, 0), (2, 0), (3, 1), (3, 2), (3, 3)])),\n",
       " (1,\n",
       "  ([(0, 2), (3, 1), (2, 2), (3, 3)],\n",
       "   [(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.groupByKey().map(lambda x:(x[0],tuple(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> groupBykey does not ALWAYS keep keys order and elements order for the same key !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths=path.map(lambda x:(x[0],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (1, 4), (1, 5), (0, 4), (0, 6)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> reduce:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 23)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.reduce(lambda (k1,v1),(k2,v2):(min(k1,k2),(v1+v2)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> reduceByKey: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applies a function to elements which have the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 14), (1, 9)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> reduceByKey is a __TRANSFORMATION__, while reduce is an __ACTION__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> combineByKey: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the last step to get the centroids is to compute the mean of the elements having the same key in the following RDD:\n",
    "for_centroids=sc.parallelize([(0, DenseVector([15151.0, 25463.0, 35000.0, 45000.0])),\n",
    "                              (1, DenseVector([61612.0, 68645.0, 61600.0, 55000.0])),\n",
    "                              (1, DenseVector([1615.0, 5463.0, 15151.0, 25463.0])),\n",
    "                              (0, DenseVector([25000.0, 15000.0, 10000.0, 5000.0])),\n",
    "                              (0, DenseVector([151.0, 263.0, 342.0, 512.0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step1: to compute the average, we compute the sum and the number of elements having the same key:\n",
    "combined = for_centroids.combineByKey(lambda value: (value, 1), \n",
    "                                      # initialisation: (key,value) ==> (key,(value,1))\n",
    "                                      lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                                      #aggregation par noeud:  (key,(values)) ==>(key,(somme(values)),count(values))\n",
    "                                      lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "                                      # regroupement des resulats de differents noeuds: \n",
    "                                      #(key,(somme_node1(values),count_node1(values))),(key,(somme_node2(values),count_node2(values)))\n",
    "                                      #    ==>key,(somme(values)),count(values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (DenseVector([40302.0, 40726.0, 45342.0, 50512.0]), 3)),\n",
       " (1, (DenseVector([63227.0, 74108.0, 76751.0, 80463.0]), 2))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step2: avg= sum(elements)/count(elements)\n",
    "computed_centroids = combined.map(lambda (key, (value_sum, count)): (key, value_sum / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       " (1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we finally get the centroids\n",
    "computed_centroids.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       " (1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> groupByKey/map is __to avoid__ (all the key-value pairs are shuffled around). Use of reduceByKey or combineByKey instead (combine output with a common key on each partition before shuffling the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4), (1, 4), (1, 5), (0, 4), (0, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(lengths.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 14), (1, 9)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.groupByKey().map(lambda x:(x[0],sum(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 14), (1, 9)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get the same result using reduceByKey \n",
    "lengths.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> aggregate:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqOp= lambda data, item: (data[0] + [item[0]], data[1] + item[1])\n",
    "combOp= lambda d1, d2: (d1[0] + d2[0], d1[1] + d2[1])\n",
    "y= lengths.aggregate(([], 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 1, 1, 0, 0], 23)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> zipWithIndex: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# element ==> (element,index_of_element)\n",
    "indexed_elements = elements.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(DenseVector([15151.0, 25463.0, 35000.0, 45000.0]), 0),\n",
       " (DenseVector([61612.0, 68645.0, 61600.0, 55000.0]), 1),\n",
       " (DenseVector([1615.0, 5463.0, 15151.0, 25463.0]), 2),\n",
       " (DenseVector([25000.0, 15000.0, 10000.0, 5000.0]), 3),\n",
       " (DenseVector([151.0, 263.0, 342.0, 512.0]), 4)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_elements.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make the index in first position\n",
    "indexed_elements=indexed_elements.map(lambda x:tuple(reversed(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([15151.0, 25463.0, 35000.0, 45000.0])),\n",
       " (1, DenseVector([61612.0, 68645.0, 61600.0, 55000.0])),\n",
       " (2, DenseVector([1615.0, 5463.0, 15151.0, 25463.0])),\n",
       " (3, DenseVector([25000.0, 15000.0, 10000.0, 5000.0])),\n",
       " (4, DenseVector([151.0, 263.0, 342.0, 512.0]))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_elements.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Functions for 2+ RDDs: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> zip:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([15151.0, 25463.0, 35000.0, 45000.0])),\n",
       " (1, DenseVector([61612.0, 68645.0, 61600.0, 55000.0])),\n",
       " (2, DenseVector([1615.0, 5463.0, 15151.0, 25463.0])),\n",
       " (3, DenseVector([25000.0, 15000.0, 10000.0, 5000.0])),\n",
       " (4, DenseVector([151.0, 263.0, 342.0, 512.0]))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zips two RDDs WITH THE SAME NUMBER OF PARTITIONS AND THE SAME NUMBER OF ELEMENTS IN EACH PARTITION\n",
    "indexes=sc.parallelize([i for i in range(5)],5)\n",
    "indexes.zip(elements).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> zip can only zip RDDs which has the __same number of partitions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes_1=sc.parallelize([i for i in range(5)],4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the number of partitions in RDD\n",
    "elements.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes_1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can only zip with RDD which has the same number of partitions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-27eebc6493bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindexes_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melements\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-2.2.1-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mzip\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   2127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2129\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can only zip with RDD which has the same number of partitions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m         \u001b[1;31m# There will be an Exception in JVM if there are different number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can only zip with RDD which has the same number of partitions"
     ]
    }
   ],
   "source": [
    "indexes_1.zip(elements).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'> cartesian:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns RDD of [((i,j) for i in RDD I) for j in RDD J]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       " (1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(0, 1), (1, 1), (2, 2), (3, 3)]),\n",
       " (1, [(0, 2), (3, 1), (2, 2), (3, 3)]),\n",
       " (1, [(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)]),\n",
       " (0, [(1, 0), (0, 1), (2, 2), (3, 3)]),\n",
       " (0, [(0, 1), (1, 0), (2, 0), (3, 1), (3, 2), (3, 3)])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       "  (0, [(0, 1), (1, 1), (2, 2), (3, 3)])),\n",
       " ((0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       "  (1, [(0, 2), (3, 1), (2, 2), (3, 3)])),\n",
       " ((0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       "  (1, [(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)])),\n",
       " ((0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       "  (0, [(1, 0), (0, 1), (2, 2), (3, 3)])),\n",
       " ((0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       "  (0, [(0, 1), (1, 0), (2, 0), (3, 1), (3, 2), (3, 3)])),\n",
       " ((1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5])),\n",
       "  (0, [(0, 1), (1, 1), (2, 2), (3, 3)])),\n",
       " ((1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5])),\n",
       "  (1, [(0, 2), (3, 1), (2, 2), (3, 3)])),\n",
       " ((1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5])),\n",
       "  (1, [(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)])),\n",
       " ((1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5])),\n",
       "  (0, [(1, 0), (0, 1), (2, 2), (3, 3)])),\n",
       " ((1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5])),\n",
       "  (0, [(0, 1), (1, 0), (2, 0), (3, 1), (3, 2), (3, 3)]))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns RDD of [((i,j) for i in RDD I) for j in RDD J]\n",
    "centroids.cartesian(path).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'>join:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join two RDDs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join two RDDs \n",
    "J=path.join(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  ([(0, 2), (3, 1), (2, 2), (3, 3)],\n",
       "   DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))),\n",
       " (1,\n",
       "  ([(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)],\n",
       "   DenseVector([31613.5, 37054.0, 38375.5, 40231.5])))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> joined RDD have __different number of partitions__ (sum of partitions ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='blue'>glom()</font> flattens elements on the same partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [(1,\n",
       "   ([(0, 2), (3, 1), (2, 2), (3, 3)],\n",
       "    DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))),\n",
       "  (1,\n",
       "   ([(0, 2), (2, 1), (1, 2), (2, 3), (3, 0)],\n",
       "    DenseVector([31613.5, 37054.0, 38375.5, 40231.5])))],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark iteration time increasing exponentially when using join:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark2-submit --conf spark.default.parallelism=20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> we can apply <font color ='blue'>leftOuterJoin</font>, <font color ='blue'>rightOuterJoin</font>, <font color ='blue'>fullOuterJoin</font>,<font color ='blue'>cogroup</font> to two RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "difference between <font color ='blue'>cogroup</font> and <font color ='blue'>fullOuterJoin</font>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1], [2]), ('b', [4], [])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cogroup(y).map(lambda x:(x[0],list(x[1][0]),list(x[1][1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('b', (4, None))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.fullOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue'>union:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appends one RDD to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))]\n"
     ]
    }
   ],
   "source": [
    "print(c1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([15151.0, 25463.0, 35000.0, 45000.0])),\n",
       " (1, DenseVector([61612.0, 68645.0, 61600.0, 55000.0])),\n",
       " (2, DenseVector([1615.0, 5463.0, 15151.0, 25463.0])),\n",
       " (3, DenseVector([25000.0, 15000.0, 10000.0, 5000.0])),\n",
       " (4, DenseVector([151.0, 263.0, 342.0, 512.0]))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_elements.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5])),\n",
       " (0, DenseVector([15151.0, 25463.0, 35000.0, 45000.0])),\n",
       " (1, DenseVector([61612.0, 68645.0, 61600.0, 55000.0])),\n",
       " (2, DenseVector([1615.0, 5463.0, 15151.0, 25463.0])),\n",
       " (3, DenseVector([25000.0, 15000.0, 10000.0, 5000.0])),\n",
       " (4, DenseVector([151.0, 263.0, 342.0, 512.0]))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.union(indexed_elements).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='#E2AC00'> count()</font> returns the number of elements in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count returns the number of elements in RDD\n",
    "c1.union(indexed_elements).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> Sometimes we need to concatenate RDD1 and RDD2 to get an RDD with RDD1 as first element and RDD2 as second element, in this case, union won't help, use instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Repartitioning: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color ='blue'> repartition:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = sc.parallelize([i for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=l.repartition(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color ='blue'> coalesce:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a new RDD which is reduced to a smaller number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=l.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> <br>\n",
    "    if you are __increasing__ the number of partitions use __repartition()__(performing full shuffle) <br>\n",
    "    if you are __decreasing__ the number of partitions use __coalesce()__ (minimizes shuffles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color ='blue'> partitionBy:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=l.map(lambda x:(x,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=p.partitionBy(2, lambda x:x%2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)],\n",
       " [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> partitionBy is applied to data with a (key,value) format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Maths: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (1, 4), (1, 5), (0, 4), (0, 6)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> max:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the max of RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> If the elements of RDD are tuples,default key for max is the first one , we can change it as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 6)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.max(key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> sum:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the sum of RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.map(lambda x:x[1]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> mean:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the mean of RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.map(lambda x:x[1]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> countByKey:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a dictionary of (key,count(elements(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {0: 3, 1: 2})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> countByValue:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a dictionary of (value,count(elements(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(0, 4): 2, (0, 6): 1, (1, 4): 1, (1, 5): 1})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Cache RDD: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if many actions will be done over an RDD cache it in memory using cache() or persist(), to remove it from memory, use unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cache() stores data in memory, with persist you can choose the level of storage (memory or disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> Dont spill to disk unless functions that computed the datasets are very expensive or they filter a large amount of data. (recomputing may be as fast as reading from disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>When RDD is shuffled? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you know if a shuffle will be called on a Transformation? <br>\n",
    "       \n",
    "       -repartition , join, cogroup, and any of the *By or *ByKeytransformations can result in shuffles\n",
    "       -If you declare a numPartitionsparameter, itll probably shuffle\n",
    "       -If a transformation constructs a shuffledRDD, itll probably shuffle\n",
    "       -combineByKeycalls a shuffle (so do other transformations like groupByKey, which actually end up calling combineByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Check if variable is RDD: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(elements,RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Save RDD: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='#E2AC00'> saveAsPickleFile:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed_elements.saveAsPickleFile('elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> we can use <font color ='#E2AC00'> saveAsTextFile</font>, <font color ='#E2AC00'> saveAsSequenceFile</font>, <font color ='#E2AC00'> saveAsHadoopFile</font>, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Load RDD: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_elements=sc.pickleFile('elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([15151.0, 25463.0, 35000.0, 45000.0])),\n",
       " (1, DenseVector([61612.0, 68645.0, 61600.0, 55000.0])),\n",
       " (2, DenseVector([1615.0, 5463.0, 15151.0, 25463.0])),\n",
       " (3, DenseVector([25000.0, 15000.0, 10000.0, 5000.0])),\n",
       " (4, DenseVector([151.0, 263.0, 342.0, 512.0]))]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_elements.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([15151.0, 25463.0, 35000.0, 45000.0])),\n",
       " (1, DenseVector([61612.0, 68645.0, 61600.0, 55000.0])),\n",
       " (2, DenseVector([1615.0, 5463.0, 15151.0, 25463.0])),\n",
       " (3, DenseVector([25000.0, 15000.0, 10000.0, 5000.0])),\n",
       " (4, DenseVector([151.0, 263.0, 342.0, 512.0]))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_elements.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Creating DataFrames: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext, Row, DataFrame, Column, GroupedData, DataFrameNaFunctions, DataFrameStatFunctions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import split, explode\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='purple'> RDD to DF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       " (1, DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|id_centroid|               value|\n",
      "+-----------+--------------------+\n",
      "|          0|[13434.0,13575.33...|\n",
      "|          1|[31613.5,37054.0,...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df =centroids.toDF([\"id_centroid\",\"value\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='purple'> read DF from csv:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+\n",
      "|pdl|           timestamp|    value|\n",
      "+---+--------------------+---------+\n",
      "|  0|2016-01-01T00:00:...|47208.477|\n",
      "|  0|2016-01-01T00:30:...| 44341.35|\n",
      "|  0|2016-01-01T01:00:...| 46310.42|\n",
      "|  0|2016-01-01T01:30:...|46844.215|\n",
      "|  0|2016-01-01T02:00:...|48449.027|\n",
      "|  0|2016-01-01T02:30:...|47331.043|\n",
      "|  0|2016-01-01T03:00:...|47770.906|\n",
      "|  0|2016-01-01T03:30:...| 47255.44|\n",
      "|  0|2016-01-01T04:00:...| 47726.23|\n",
      "|  0|2016-01-01T04:30:...|44565.863|\n",
      "|  0|2016-01-01T05:00:...|24933.562|\n",
      "|  0|2016-01-01T05:30:...|25545.246|\n",
      "|  0|2016-01-01T06:00:...| 26215.35|\n",
      "|  0|2016-01-01T06:30:...|26145.475|\n",
      "|  0|2016-01-01T07:00:...|25813.287|\n",
      "|  0|2016-01-01T07:30:...|24585.338|\n",
      "|  0|2016-01-01T08:00:...|21934.707|\n",
      "|  0|2016-01-01T08:30:...|21747.996|\n",
      "|  0|2016-01-01T09:00:...|21209.621|\n",
      "|  0|2016-01-01T09:30:...| 20941.58|\n",
      "+---+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the file centroids.csv\n",
    "filepath=\"centroids.csv\"\n",
    "# define the schema of the dataframe\n",
    "schema = StructType([StructField(\"pdl\", StringType(), True), \n",
    "                     StructField(\"timestamp\", StringType(), True), \n",
    "                     StructField(\"value\", FloatType(), True)])\n",
    "\n",
    "df2 = sqlContext.read.csv(filepath, header=True, schema=schema, sep=';')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='purple'> read DF from Hive:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query=\"select * from hive table \"\n",
    "# df = sqlContext.sql(query);\n",
    "\n",
    "# df.registerTempTable(\"temp_table\");\n",
    "# df = sqlContext.sql(\"select * from temp_table\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'> DataFrame to RDD:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_centroid=0, value=DenseVector([13434.0, 13575.3333, 15114.0, 16837.3333])),\n",
       " Row(id_centroid=1, value=DenseVector([31613.5, 37054.0, 38375.5, 40231.5]))]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'> Save DataFrame:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='purple'> save DF to csv:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path=\"saved_data_frame\"\n",
    "# for debug we can use overwrite mode\n",
    "# df.write.options(header=\"true\", sep=';').mode(\"overwrite\").csv(filename)\n",
    "df.write.options(header=\"true\", sep=';').csv(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> NOTICE: </font> write csv returns a __directory__ containing n csv files, n=numpartions(dataframe). To save dataframe into a single csv file, consider doing coalesce(1) before saving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='purple'> save RDD/DF to hive table:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Define SCHEMA TO OUR TABLE\n",
    "# schema = StructType([StructField(\"id_centroid\", StringType(), True), \n",
    "#                      StructField(\"timestamp\", StringType(), True), \n",
    "#                      StructField(\"value\", FloatType(), True)])\n",
    "# # RDD to DF\n",
    "# df_hive = sqlContext.createDataFrame(rdd_hive, schema)\n",
    "# df_hive = df_hive.select(df_hive.id_centroid, to_timestamp(df_hive.timestamp).alias('timestamp'),df_hive.value)\n",
    "# # Coalesce DF to one partition so that we save it into one table        \n",
    "# df_hive = df_hive.coalesce(1)\n",
    "# # Save DF to hive table\n",
    "# df_hive.registerTempTable(\"tablename\")\n",
    " \n",
    "# # df_hive = sqlContext.sql(\"insert into common.table select * from df_hive\")\n",
    "# df_hive.write.mode(\"append\").insertInto(\"common.table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'> Operations on DataFrame:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='purple'> withColumn:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Transform colum timestamp\n",
    "# regex = r\"^(\\d{4})-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2}):(\\d{2})$\"\n",
    "# sub = r\"$1$2$3$4$5$6\"\n",
    "# df3 = df2.withColumn('timestamp', F.regexp_replace('timestamp', regex, sub))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Stopping SparkContext: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stop SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Execution of python file from a shell : </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark2-submit --master yarn --driver-cores 6 --driver-memory 30G --num-executors 10 --executor-cores 4 --executor-memory 4G --queue root.projet2 /directory/code.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Execution of python file from a shell on remote server: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nohup spark2-submit --master yarn --driver-cores 6 --driver-memory 30G --num-executors 10 --executor-cores 4 --executor-memory 4G --queue root.projet2 /directory/code.py &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green'>Run spark from spyder: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark-submit C:\\ProgramData\\Anaconda2\\Scripts\\spyder-script.py  &spark-submit C:\\ProgramData\\Anaconda2\\Scripts\\spyder-script.py &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark applications in Python can either be run with the bin/spark-submit script which includes Spark at runtime, or by including including it in your setup.py as:\n",
    "\n",
    "        install_requires=[\n",
    "        'pyspark=={site.SPARK_VERSION}'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liens utils: <br>\n",
    "https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c <br>\n",
    "https://spark-packages.org/ <br>\n",
    "http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism <br>\n",
    "https://www.youtube.com/watch?v=7ooZ4S7Ay6Y&feature=youtu.be <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
